---
title: 6 June 2025
date: 2025-06-06
type: treadmill
---

## News

### OpenAI updates

OpenAI released (native?) audio models (gpt-4o-audio-preview-2025-06-03 and gpt-4o-realtime-preview-2025-06-03) and o3-pro.

[Codex received updates](https://x.com/i/web/status/1929956778105811071), including availability for Plus accounts, internet access for dependency updates or integration tests ([warning: potential security risk](https://platform.openai.com/docs/codex/agent-network)), and some usability updates.

Additionally, [memory is coming to free ChatGPT accounts](https://x.com/i/web/status/1929937841905381558).

### Anthropic News

[Claude Explains \\ Anthropic](https://www.anthropic.com/claude-explains) is an AI-generated technical blog with human editorial overview.

[Claude Code is now available with Pro plans](https://support.anthropic.com/en/articles/11145838-using-claude-code-with-your-pro-or-max-plan). Claude Code has rate limits on a rolling 5-hour time window; Pro plans can use ~10-40 prompts every 5 hours, while Max plans can use up to 900 prompts in the same time window. These prompt counts are estimates based on token allotments. As an aside, Simon Willison shared [claude-trace](https://simonwillison.net/2025/Jun/2/claude-trace/), which exposes traces of how Claude Code works, and finds that it uses a `dispatch_agent` to delegate work to multiple agents running concurrently to accomplish the initial user request.

### [Gemini 2.5 Pro: Access Google's latest preview AI model](https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/)

Logan Kilpatrick released the updated `gemini-2.5-pro-preview-06-05` at AI Engineering World Fair and promised more models (including embedding models) coming soon.

### [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models | Qwen](https://qwenlm.github.io/blog/qwen3-embedding/)

Qwen (from Alibaba) released their [Embedding](https://huggingface.co/collections/Qwen/qwen3-embedding-6841b2055b99c44d9a4c371f) and [Reranking](https://huggingface.co/collections/Qwen/qwen3-reranker-6841b22d0192d7ade9cdefea) model families based on Qwen 3. These are very impressive models, outperforming peers from Cohere, OpenAI, Jina, etc. These are long-context (32k), multilingual models

### [Introducing Copilot Spaces: A new way to work with code and context - GitHub Changelog](https://github.blog/changelog/2025-05-29-introducing-copilot-spaces-a-new-way-to-work-with-code-and-context/)

Copilot Spaces is a way to create curated-context workspaces on GitHub's website, allowing teams to centralize knowledge, share expertise, and direct AI assistants who can act on the available context.

### [Meta buys a nuclear power plant (more or less) | TechCrunch](https://techcrunch.com/2025/06/03/meta-buys-a-nuclear-power-plant-more-or-less/)

Meta joins [Microsoft (Three Mile Island)](https://techcrunch.com/2024/09/20/microsoft-taps-three-mile-island-nuclear-plant-to-power-ai/) and [Amazon (Susquehanna)](https://electrek.co/2024/03/05/amazon-just-bought-a-100-nuclear-powered-data-center/) in powering datacenters with nuclear power.

## Ideas

### Swyx's keynote from AI Engineering Conference [slides | google docs](https://docs.google.com/presentation/d/1KCr_QYDlN37JnCxirufMvDmodpuJ3I_KtuqEZBik2eE/edit?pli=1&slide=id.p1#slide=id.p1)

What is the new Standard Model for AI Engineering?

```md
Data Engineering -> ETL
Backend -> CRUD
Machine Learning -> Train - Validate - Test
...
AI:
  Agents -> [IMPACT](https://www.latent.space/p/agent) (Intent, Memory, Planning, (delegated) Authority, Control flow, Tools)
  Workflows -> SPADE (Sync (data), Plan / (parallel) Process, Analyze / Act, Deliver, Evaluate)
```

### [Designing a next-generation reasoning model](https://www.interconnects.ai/p/next-gen-reasoners)

Next-gen reasoning models (going beyond simple inference-time scaling like OpenAI o1, Deepseek R-1) will require 4 aspects:

1. Skills: The ability to _plan_ and solve self-contained problems (including using tools within the reasoning loop).
2. Calibration: The ability to understand the difficulty of a problem and not overthink.
3. Strategy: The ability to choose the right high level plan.
4. Abstraction: The ability to break down a strategy into solvable chunks.

### [Large language models are fundamentally unreliable](https://verissimo.substack.com/p/verissimo-monthly-may-2025)

> The unreliability of LLMs isn't just a bug, it's the foundational constraint that defines what's worth building, how it should be built, and what defensibility even looks like in this new wave of AI-native products. (Indeed, one could argue that unreliability is the raison d'être for much of the application layer, it's the challenge that creates the opportunity for builders to deliver value.)... The best builders aren't trying to beat the model. They're building systems that expect it to fail, and still work anyway.

### [Prompt Engineering is Dead - Everything is a Spec | AI Engineer World Fair](https://youtu.be/8rABwKRsec4)

Sean Grove's talk "Prompt Engineering is Dead - Everything is a Spec" highlighted the idea that code is the artifact of the value-work that programmers do, but not the value itself (the value is what the code is used for); and the value of the programmer is navigating lossy communication to align the artifact (output) with intention (input).

Specs (specifications) are designed to do exactly that; the US constitution is a spec for government, laws are a spec for society, OpenAI's `model_spec` is a spec for AI behavioral expectation. Specs align humans by acting as a living record of intent; since LLMs are text-native, they can also benefit from this alignment of intention.

Grove asked why we don't preserve prompts, when they're the record of our intent for the model's generation of an artifact. Consider compilation -> binary -- we don't throw away the source code once we have the binary. Why do we throw away the prompt that got us the code when using AI assistance to code? Therefore, it would be better to preserve our prompts just as we preserve the code, so we can understand how we got the result.

Writing specs that fully capture the intent and values is the supercritical job; programmers, Product Managers, and Project Managers that master this will massively outperform their peers in the world where AI delegation is a primary component of your job.

### Communication and the New Developer Workflow

I came across two articles ([LLMs are mirrors of operator skill](https://ghuntley.com/mirrors/), [What Actually Works: 12 Lessons from AI Pair Programming | Forge Code](https://forgecode.dev/blog/ai-agent-best-practices/)) that discuss the new programming paradigm (working with AI) and discuss similar (current) best practices for working with AI in development.

01. Write a plan first, let AI critique it before coding
02. Use edit-test loops: write failing test → AI fixes → repeat
03. Ask for step-by-step reasoning before code
04. Curate content intentionally rather than dumping entire codebases into prompts
05. Commit small, frequent changes for readable diffs
06. Keep prompts short and specific context bloat kills accuracy
07. Re-index your project after major changes to avoid hallucinations
08. Use file references (@path/file.rs:42-88) not code dumps or copy-paste
09. AI can write tests, but you write the specs
10. Debug with requests for diagnostic report (What did you do last? What happened? How can we fix?)
11. Set clear style guidelines and expectations
12. Review like a senior engineer

### [Cloudlflare builds OAuth with Claude and publishes all the prompts](https://github.com/cloudflare/workers-oauth-provider/)

_"This library (including the schema documentation) was largely written with the help of Claude, the AI model by Anthropic. Claude's output was thoroughly reviewed by Cloudflare engineers with careful attention paid to security and compliance with standards. Many improvements were made on the initial output, mostly again by prompting Claude (and reviewing the results). Check out the commit history to see how Claude was prompted and what code it produced."_
...
_"To emphasize, this is not "vibe coded". Every line was thoroughly reviewed and cross-referenced with relevant RFCs, by security experts with previous experience with those RFCs. I was trying to validate my skepticism. I ended up proving myself wrong."_

## Research

### [[2505.24832] How much do language models memorize?](https://arxiv.org/abs/2505.24832)

Models memorize until their capacity fills, at which point 'grokking' begins and unintended memorization decreases as models begin to generalize. Measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter.

This paper makes a distinction between "memorization" (a.k.a. _unintended_ memorization) and "generalization" (a.k.a. _intended_ memorization), where the prior retains details from training that are irrelevant to the training task and the latter the concept in a way that can be generalized to other similar tasks.

Fascinatingly, the research finds that _double descent begins exactly when the data capacity exceeds the model capacity_. One theory is that once the model can no longer memorize datapoints individually, it is forced to share information between datapoints to save capacity,
which leads to generalization.

### [[2505.24362] Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)

Chain-of-Thought is powerful but can be costly. This research finds that a simple probing classifier, based on an LLM's internal representations of the text (just before it starts generating tokens), can accurately predict the success of a zero-shot CoT reasoning process. This suggests that crucial information about the reasoning's outcome is embedded in the initial steps. As a result, early stopping of reasoning when success is unlikely could lead to more efficient CoT strategies—saving resources while maintaining performance.

The "hidden state" is represented by tensor of the weights of each layer at the last token of the input prompt. A classifier is trained to predict the contribution of each layer to overall success. For the small models used in this research (Llama-3.1-8b and Mistral-7b), intermediate layers seem frequently predictive in task success (specifically layers 14 and 16), though other layers were also involved and the predictor layers will likely change with various model architectures and sizes.

### [[2505.24726] Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726)

A model's ability to solve complex, verifiable tasks can be enhanced by incentivizing the model to generate better self-reflections when it answers incorrectly, rather than training at a particular task. Models trained only at self-reflection also improved on first-shot performance (they succeeded and did not need to self-reflect to try again). The research framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded.

### [[2505.17117] From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
